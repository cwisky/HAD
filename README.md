# HAD
* HAD(Human Action Detection)
## Table of Contents
<ol>
  <li>Github에 저장된 소스코드와 데이터, 이미지 링크 </li>
  <li>미디어파이프를 사용하여 mp4 비디오로부터 관절정보 추출하기(Python)</li>
  <li>추출된 관절 정보(CSV) </li>
  <li>Blender를 이용한 mp4 비디오의 좌우반전(Flipping, Blender Video Editing) </li>
  <li>Blender를 사용하여 mp4 비디오 크기 및 렌더링 속성 변환하기(Blender Output Properties) </li>
  <li>Blender에서 애니메이션의 키프레임에 접근하여 관절의 회전량 조작하기(Python) </li>
  <li>Blender 애니메이션을 mp4 비디오로 변환하기(Blender Output Properties) </li>
  <li>Blender mp4비디오의 해상도, 프레임수, 좌우반전(Blender Output Properties) </li>
  <li>Google colab에서 mp4 비디오 데이터 학습(Model 1, Python) </li>
  <li>Google colab에서 mp4 비디오 데이터 학습(Model 2, Model 3, Python) </li>
  <li>모델검증 측정지표(그래픽) </li>
  <li>모델 검증에 사용된 150개 비디오 압축파일(real_vids_01.zip, real_vids_02.zip, real_vids_03.zip) </li>
  <li>LSTM 모델 시계열 데이터 전처리(Window Sliding) 이미지 및 코드 </li>
  <li>Mediapipe 관절정보 추출 대상 주요 관절이름 </li>
  <li>원본 비디오 학습용으로 사용된 비디오(mp4):걷기, 달리기, 반전된 비디오 2개(총 4개) </li>
  <li>학습된 LSTM 모델 3개(Model 1, Model 2, Model 3)</li>
</ol>
